{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e5b2dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6ba31ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zScoreScale(x):\n",
    "    data = x\n",
    "    mean = data.mean()\n",
    "    sig = np.sqrt(((data - mean)**2).sum())\n",
    "    scaled_x = np.zeros(x.shape)\n",
    "    \n",
    "    scaled_data = (data - mean)/sig\n",
    "    for i in range(x.shape[1]):\n",
    "        scaled_x[:,i] = scaled_data[:,i]\n",
    "    return scaled_x, mean, sig\n",
    "\n",
    "def zScoreDescale(x, mean, sig):\n",
    "    scaled_data = x\n",
    "    descaled_data = scaled_data * sig + mean\n",
    "    \n",
    "    descaled_x = np.zeros(x.shape)\n",
    "    for i in range(x.shape[1]):\n",
    "        descaled_x[:,i] = descaled_data[:,i]\n",
    "        \n",
    "    return descaled_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "6c1757a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticalRegression:\n",
    "    \n",
    "    # Construtor\n",
    "    def __init__(self, x, y, addOnes = True):\n",
    "        if addOnes:\n",
    "            self.x = np.c_[np.ones(x.shape[0]), x]\n",
    "        else:\n",
    "            self.x = x\n",
    "        self.y = y\n",
    "        self.w = np.zeros(self.x.shape[1]).reshape(-1,1)\n",
    "        self.MSE = 0.0\n",
    "    \n",
    "    # Getters\n",
    "    def getX(self):\n",
    "        return self.x\n",
    "    \n",
    "    def getY(self):\n",
    "        return self.y\n",
    "    \n",
    "    def getW(self):\n",
    "        return self.w\n",
    "    \n",
    "    def getMSE(self):\n",
    "        return self.MSE\n",
    "    \n",
    "    # Setters\n",
    "    def setX(self, x, addOnes = True):\n",
    "        if addOnes:\n",
    "            x = np.c_[np.ones(x.shape[0]), x]\n",
    "        self.x = x\n",
    "        self.w = np.zeros(self.x.shape[1]).reshape(-1,1)\n",
    "    \n",
    "    def setY(self, y):\n",
    "        self.y = y\n",
    "    \n",
    "    # Métodos\n",
    "    def sigmoid(self, z):\n",
    "        return (1/(1 + np.exp(-1 * z)))\n",
    "    \n",
    "    def trainGD(self, alpha = 0.1, l2 = 0.01, max_iterations = 100):\n",
    "        \n",
    "        n = len(self.y)\n",
    "        yhat = np.zeros(n).reshape(-1,1)\n",
    "        e = np.zeros(n).reshape(-1,1)\n",
    "        \n",
    "        for t in range(max_iterations):\n",
    "            \n",
    "            yhat = self.sigmoid(self.x @ self.w)\n",
    "            e = self.y - yhat\n",
    "            \n",
    "            self.w[0] = self.w[0] + (alpha/n * e.sum())\n",
    "            for column in range(1, len(self.w)):\n",
    "                self.w[column] = self.w[column] + alpha * ((e * self.x[:,[column]]).sum()/n - l2 * self.w[column])  \n",
    "        \n",
    "        self.MSE = ((e ** 2).sum())/(2*n)\n",
    "    \n",
    "    def test(self):\n",
    "        return self.y - (self.x @ self.w)\n",
    "    \n",
    "    def predict(self, x, addOnes = True):\n",
    "        if addOnes:\n",
    "            x = np.c_[np.ones(x.shape[0]), x]\n",
    "        y_pred = np.sign(x @ self.w)\n",
    "        y_pred[y_pred == -1] = 0\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "af3140bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassLogisticalRegression:\n",
    "    \n",
    "    # Construtor\n",
    "    def __init__(self, x, y, addOnes = True):\n",
    "        if addOnes:\n",
    "            self.x = np.c_[np.ones(x.shape[0]), x]\n",
    "        else:\n",
    "            self.x = x\n",
    "        self.y = y\n",
    "        self.w = np.zeros((self.x.shape[1], y.shape[1]))\n",
    "    \n",
    "    # Getters\n",
    "    def getX(self):\n",
    "        return self.x\n",
    "    \n",
    "    def getY(self):\n",
    "        return self.y\n",
    "    \n",
    "    def getW(self):\n",
    "        return self.w\n",
    "    \n",
    "    def getMSE(self):\n",
    "        return self.MSE\n",
    "    \n",
    "    # Setters\n",
    "    def setXY(self, x, y, addOnes = True):\n",
    "        if addOnes:\n",
    "            x = np.c_[np.ones(x.shape[0]), x]\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.w = np.zeros((self.x.shape[1], y.shape[1]))\n",
    "    \n",
    "    # Métodos\n",
    "    def softmax(self, w, x):\n",
    "        numerator = np.exp(x @ w)\n",
    "        denominator = np.sum(np.exp(x @ w), axis=1).reshape(-1,1)\n",
    "        return numerator/denominator\n",
    "    \n",
    "    def trainGD(self, alpha = 0.1, max_iterations = 100):\n",
    "    \n",
    "        n = self.y.shape[0]\n",
    "        yhat = np.zeros(self.y.shape)\n",
    "        e = np.zeros(self.y.shape)\n",
    "                 \n",
    "        for t in range(max_iterations):\n",
    "            yhat = self.softmax(self.w, self.x)\n",
    "            e = self.y - yhat\n",
    "            \n",
    "            self.w = self.w + (alpha/n) * self.x.T @ e\n",
    "    \n",
    "    def test(self):\n",
    "        return self.y - (self.x @ self.w)\n",
    "    \n",
    "    def predict(self, x, addOnes = True):\n",
    "        if addOnes:\n",
    "            x = np.c_[np.ones(x.shape[0]), x]\n",
    "            \n",
    "        probabilities = self.softmax(self.w, x)\n",
    "        max_indexes = np.argmax(probabilities, axis=1)\n",
    "        \n",
    "        prediction = np.zeros(probabilities.shape)\n",
    "        prediction[np.arange(len(max_indexes)), max_indexes] = 1\n",
    "        \n",
    "        print(prediction)\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "769ae41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesGaussiano:\n",
    "    \n",
    "    # Construtor \n",
    "    def __init__(self, x, y):\n",
    "        self.classes = np.eye(y.shape[1])\n",
    "        self.n_per_class = np.zeros((y.shape[1], 1))\n",
    "        self.class_priors = np.zeros((y.shape[1], 1)) # Probabilidade de cada classe\n",
    "        self.means = np.zeros((self.classes.shape[1], x.shape[1]))\n",
    "        self.variances = np.zeros((self.classes.shape[1], x.shape[1], x.shape[1]))\n",
    "        \n",
    "        # Calcula n_per_class\n",
    "        for line in y:\n",
    "            for i in range(0, len(line)):\n",
    "                if line[i] == 1:\n",
    "                    self.n_per_class[i] += 1\n",
    "                    break\n",
    "        \n",
    "        # Calcula class_priors\n",
    "        for i in range(0, len(self.class_priors)):\n",
    "            self.class_priors[i] = self.n_per_class[i]/self.n_per_class.sum()\n",
    "        \n",
    "        # Calcula a média de cada atributo para cada classe\n",
    "        for i in range(0, y.shape[0]):\n",
    "            for c in range(self.classes.shape[0]):\n",
    "                if np.array_equal(y[i], self.classes[c]):\n",
    "                    self.means[c] += x[i]\n",
    "                    break      \n",
    "        for clss in range(0, y.shape[1]):\n",
    "            self.means[clss, :] = self.means[clss, :]/self.n_per_class[clss]\n",
    "    \n",
    "        # Calcula a variância de cada feature\n",
    "        for clss in range(self.classes.shape[0]):\n",
    "            for feature in range(x.shape[1]):\n",
    "                self.variances[clss, feature, feature] = np.sum((x[y[:, clss] == 1, feature] - self.means[clss, feature])**2)/(self.n_per_class[clss] - 1)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        probs = np.zeros((x.shape[0], self.classes.shape[0]))\n",
    "        y_pred = np.zeros(probs.shape)\n",
    "        for i in range(x.shape[0]):\n",
    "            for clss in range(self.classes.shape[0]):\n",
    "                probs[i, clss] = np.log(self.class_priors[clss])\n",
    "                probs[i, clss] -= 0.5 * np.sum(np.log(2 * np.pi * self.variances[clss, :].sum()))\n",
    "                probs[i, clss] -= 0.5 * np.sum(((x[i] - self.means[clss, :])**2)/self.variances[clss, :].sum())\n",
    "        for i in range(x.shape[0]):\n",
    "            y_pred[i] = self.classes[np.argmax(probs[i])]\n",
    "    \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "4f49cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminanteGaussiano:\n",
    "    \n",
    "    # Construtor \n",
    "    def __init__(self, x, y):\n",
    "        self.classes = np.eye(y.shape[1])\n",
    "        self.n_per_class = np.zeros((y.shape[1], 1))\n",
    "        self.class_priors = np.zeros((y.shape[1], 1)) # Probabilidade de cada classe\n",
    "        self.means = np.zeros((self.classes.shape[1], x.shape[1]))\n",
    "        self.variances = np.zeros((self.classes.shape[1], x.shape[1], x.shape[1]))\n",
    "        \n",
    "        # Calcula n_per_class\n",
    "        for line in y:\n",
    "            for i in range(0, len(line)):\n",
    "                if line[i] == 1:\n",
    "                    self.n_per_class[i] += 1\n",
    "                    break\n",
    "        \n",
    "        # Calcula class_priors\n",
    "        for i in range(0, len(self.class_priors)):\n",
    "            self.class_priors[i] = self.n_per_class[i]/self.n_per_class.sum()\n",
    "        \n",
    "        # Calcula a média de cada atributo para cada classe\n",
    "        for i in range(0, y.shape[0]):\n",
    "            for c in range(self.classes.shape[0]):\n",
    "                if np.array_equal(y[i], self.classes[c]):\n",
    "                    self.means[c] += x[i]\n",
    "                    break      \n",
    "        for clss in range(0, y.shape[1]):\n",
    "            self.means[clss, :] = self.means[clss, :]/self.n_per_class[clss]\n",
    "    \n",
    "        \n",
    "        # Calcula a variância de cada feature para cada classe\n",
    "        for clss in range(self.classes.shape[0]):\n",
    "            self.variances[clss] = np.cov(x[y[:, clss] == 1, :], rowvar=False)/self.n_per_class[clss]\n",
    "            self.variances[clss] += np.identity(self.variances.shape[1]) * 1e-10\n",
    "            \n",
    "    def predict(self, x):\n",
    "        probs = np.zeros((x.shape[0], self.classes.shape[0]))\n",
    "        y_pred = np.zeros(probs.shape)\n",
    "        for i in range(x.shape[0]):\n",
    "            for clss in range(self.classes.shape[0]):\n",
    "            \n",
    "                det = np.linalg.det(self.variances[clss])\n",
    "                inv = np.linalg.inv(self.variances[clss])\n",
    "            \n",
    "                probs[i, clss] = np.log(self.class_priors[clss])\n",
    "                probs[i, clss] -= 0.5 * np.log(det)\n",
    "                probs[i, clss] -= 0.5 * (x[i] - self.means[clss, :]) @ inv @ (x[i] - self.means[clss, :]).T\n",
    "                \n",
    "        for i in range(x.shape[0]):\n",
    "            y_pred[i] = self.classes[np.argmax(probs[i])]\n",
    "            \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "df3e69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold(data):\n",
    "    np.random.seed(1200)\n",
    "    folds = []\n",
    "    splits = 10\n",
    "    indices = np.random.permutation(data.shape[0])\n",
    "    folds_idx = np.array_split(indices, splits)\n",
    "        \n",
    "    for i in range(0, splits):\n",
    "        train_idx = np.concatenate(folds_idx[:i] + folds_idx[i+1:])\n",
    "        test_idx = folds_idx[i]\n",
    "        folds.append((train_idx, test_idx))\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f800d9f2",
   "metadata": {},
   "source": [
    "# Questão 1\n",
    "\n",
    "Professor, quando fiz meu algoritmo de regressão logística binária usando GD, testei num dataset pequeno e ele funcionou sem problemas. Porém, quando testo no dataset da questão, o resultado da predição inteira é de uma classe só, com pesos bem similares. Passei horas analisando o código e não consegui encontrar o problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "a00de6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análise da Regressão Logística(GD):\n",
      "\n",
      "Média de acurácia:                  0.5\n",
      "Desvio padrão da acurácia:          0.5\n",
      "Média da classe positiva:           0.0\n",
      "Desvio padrão da classe positiva:   0.0\n",
      "Média da classe Negativa:           1.0\n",
      "Desvio padrão da classe positiva:   0.0\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Análise do Naive Bayes:\n",
      "\n",
      "Média de acurácia:                  0.8849795249573799\n",
      "Desvio padrão da acurácia:          0.1103959596017686\n",
      "Média da classe positiva:           0.7926406316795332\n",
      "Desvio padrão da classe positiva:   0.081765808487049\n",
      "Média da classe Negativa:           0.9773184182352264\n",
      "Desvio padrão da classe positiva:   0.02521796842917371\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Análise do discriminante gaussiano:\n",
      "\n",
      "Média de acurácia:                  0.8581028003311534\n",
      "Variância da acurácia:              0.0220137644003484\n",
      "Média da classe positiva:           1.0\n",
      "Desvio padrão da classe positiva:   0.0\n",
      "Média da classe Negativa:           0.716205600662307\n",
      "Desvio padrão da classe positiva:   0.06130169861411506\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt(\"breastcancer.csv\", delimiter=',')\n",
    "\n",
    "x = data[:, :30]\n",
    "y = data[:, 30].reshape(-1, 1)\n",
    "\n",
    "scaled_x, mean, sig = zScoreScale(x)\n",
    "\n",
    "one_hot_y = np.zeros((x.shape[0], 2))\n",
    "for l in range(len(y)):\n",
    "    if y[l] == 1:\n",
    "        one_hot_y[l] = np.array([1, 0])\n",
    "    else:\n",
    "        one_hot_y[l] = np.array([0, 1])\n",
    "\n",
    "folds = kfold(x)\n",
    "acuracias = np.zeros((3, 10, 2))\n",
    "\n",
    "for f, (train_idx, test_idx) in enumerate(folds):\n",
    "#(train_idx, test_idx) = folds[0]\n",
    "\n",
    "    x_train = scaled_x[train_idx, :]\n",
    "    y_train = y[train_idx].reshape(-1,1)\n",
    "    one_hot_y_train = one_hot_y[train_idx]\n",
    "\n",
    "    x_test = scaled_x[test_idx, :]\n",
    "    y_test = y[test_idx].reshape(-1,1)\n",
    "    one_hot_y_test = one_hot_y[test_idx]\n",
    "\n",
    "    logistical_gd = BinaryLogisticalRegression(x_train, y_train)\n",
    "    gaussian_discriminant = DiscriminanteGaussiano(x_train, one_hot_y_train)\n",
    "    gaussian_naive_bayes = NaiveBayesGaussiano(x_train, one_hot_y_train)\n",
    "\n",
    "    logistical_gd.trainGD(alpha = 0.1, l2 = 0.01, max_iterations = 1000)\n",
    "    yhat_lgd = logistical_gd.predict(x_test)\n",
    "    #print(logistical_gd.getW())\n",
    "    #print(yhat_lgd)\n",
    "\n",
    "    yhat_gnb = gaussian_naive_bayes.predict(x_test)\n",
    "    yhat_g_disc = gaussian_discriminant.predict(x_test)\n",
    "    \n",
    "    \n",
    "    n_per_class = np.zeros((one_hot_y_test.shape[1], 1))\n",
    "    for line in one_hot_y_test:\n",
    "        for i in range(0, len(line)):\n",
    "            if line[i] == 1:\n",
    "                n_per_class[i] += 1\n",
    "                break\n",
    "    \n",
    "    for i, clss in enumerate(one_hot_y_test):\n",
    "        if ((yhat_lgd[i] == 0) and (clss[1] == 1)) or ((yhat_lgd[i] == 1) and (clss[0] == 1)):\n",
    "            if clss[0] == 1:\n",
    "                acuracias[0, f, 0] += 1\n",
    "            else:\n",
    "                acuracias[0, f, 1] += 1\n",
    "        \n",
    "        if np.array_equal(clss, yhat_gnb[i]):\n",
    "            if clss[0] == 1:\n",
    "                acuracias[1, f, 0] += 1\n",
    "            else:\n",
    "                acuracias[1, f, 1] += 1\n",
    "                \n",
    "        if np.array_equal(clss, yhat_g_disc[i]):\n",
    "            if clss[0] == 1:\n",
    "                acuracias[2, f, 0] += 1\n",
    "            else:\n",
    "                acuracias[2, f, 1] += 1\n",
    "    \n",
    "    acuracias[0, f, 0] /= n_per_class[0]\n",
    "    acuracias[0, f, 1] /= n_per_class[1]\n",
    "    acuracias[1, f, 0] /= n_per_class[0]\n",
    "    acuracias[1, f, 1] /= n_per_class[1]\n",
    "    acuracias[2, f, 0] /= n_per_class[0]\n",
    "    acuracias[2, f, 1] /= n_per_class[1]\n",
    "\n",
    "#print(acuracias)\n",
    "\n",
    "print(\"Análise da Regressão Logística(GD):\\n\")\n",
    "print(\"Média de acurácia:                 \", acuracias[0].mean())\n",
    "print(\"Desvio padrão da acurácia:         \", acuracias[0].std())\n",
    "print(\"Média da classe positiva:          \", acuracias[0, :, 0].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[0, :, 0].std())\n",
    "print(\"Média da classe Negativa:          \", acuracias[0, :, 1].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[0, :, 1].std())\n",
    "\n",
    "print(\"\\n--------------------------------------------------\\n\")\n",
    "\n",
    "print(\"Análise do Naive Bayes:\\n\")\n",
    "print(\"Média de acurácia:                 \", acuracias[1].mean())\n",
    "print(\"Desvio padrão da acurácia:         \", acuracias[1].std())\n",
    "print(\"Média da classe positiva:          \", acuracias[1, :, 0].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[1, :, 0].std())\n",
    "print(\"Média da classe Negativa:          \", acuracias[1, :, 1].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[1, :, 1].std())\n",
    "\n",
    "print(\"\\n--------------------------------------------------\\n\")\n",
    "\n",
    "print(\"Análise do discriminante gaussiano:\\n\")\n",
    "print(\"Média de acurácia:                 \", acuracias[2].mean())\n",
    "print(\"Variância da acurácia:             \", acuracias[2].var())\n",
    "print(\"Média da classe positiva:          \", acuracias[2, :, 0].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[2, :, 0].std())\n",
    "print(\"Média da classe Negativa:          \", acuracias[2, :, 1].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[2, :, 1].std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebf2fac",
   "metadata": {},
   "source": [
    "# Questão 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "e8e65426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "[[0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Análise da Regressão Softmax(GD):\n",
      "\n",
      "Média de acurácia:                  0.2713311688311688\n",
      "Desvio padrão da acurácia:          0.4222434877571003\n",
      "Média da classe positiva:           0.5253246753246753\n",
      "Desvio padrão da classe positiva:   0.45074898782504147\n",
      "Média da classe Negativa:           0.175\n",
      "Desvio padrão da classe positiva:   0.35443617196894556\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Análise do Naive Bayes:\n",
      "\n",
      "Média de acurácia:                  0.4127574385474486\n",
      "Desvio padrão da acurácia:          0.3585228258506487\n",
      "Média da classe positiva:           0.01814194577352472\n",
      "Desvio padrão da classe positiva:   0.022556552796702017\n",
      "Média da classe Negativa:           0.3857632549511591\n",
      "Desvio padrão da classe positiva:   0.08285542540852096\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Análise do Discriminante Gaussiano:\n",
      "\n",
      "Média de acurácia:                  0.7897849938249699\n",
      "Variância da acurácia:              0.05715240118837599\n",
      "Média da classe positiva:           0.9633673831042252\n",
      "Desvio padrão da classe positiva:   0.04773088883311959\n",
      "Média da classe Negativa:           0.7831841055189328\n",
      "Desvio padrão da classe positiva:   0.10612320150174107\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt(\"vehicle.csv\", delimiter=',')\n",
    "\n",
    "x = data[:, :18]\n",
    "y = data[:, 18].reshape(-1, 1)\n",
    "\n",
    "scaled_x, mean, sig = zScoreScale(x)\n",
    "\n",
    "one_hot_y = np.zeros((x.shape[0], 4))\n",
    "for l in range(len(y)):\n",
    "    match(y[l]):\n",
    "        case 0:\n",
    "            one_hot_y[l] = np.array([1, 0, 0, 0])\n",
    "        case 1:\n",
    "            one_hot_y[l] = np.array([0, 1, 0, 0])\n",
    "        case 2:\n",
    "            one_hot_y[l] = np.array([0, 0, 1, 0])\n",
    "        case 3:\n",
    "            one_hot_y[l] = np.array([0, 0, 0, 1])\n",
    "\n",
    "folds = kfold(x)\n",
    "acuracias = np.zeros((3, 10, 4))\n",
    "\n",
    "for f, (train_idx, test_idx) in enumerate(folds):\n",
    "#(train_idx, test_idx) = folds[0]\n",
    "\n",
    "    x_train = scaled_x[train_idx, :]\n",
    "    y_train = y[train_idx].reshape(-1,1)\n",
    "    one_hot_y_train = one_hot_y[train_idx]\n",
    "\n",
    "    x_test = scaled_x[test_idx, :]\n",
    "    y_test = y[test_idx].reshape(-1,1)\n",
    "    one_hot_y_test = one_hot_y[test_idx]\n",
    "\n",
    "    softmax_gd = MulticlassLogisticalRegression(x_train, one_hot_y_train)\n",
    "    gaussian_discriminant = DiscriminanteGaussiano(x_train, one_hot_y_train)\n",
    "    gaussian_naive_bayes = NaiveBayesGaussiano(x_train, one_hot_y_train)\n",
    "\n",
    "    softmax_gd.trainGD(alpha = 0.1, max_iterations = 200)\n",
    "    yhat_softmax = softmax_gd.predict(x_test)\n",
    "    #print(logistical_gd.getW())\n",
    "    #print(yhat_lgd)\n",
    "\n",
    "    yhat_gnb = gaussian_naive_bayes.predict(x_test)\n",
    "    #print(yhat_gnb)\n",
    "    yhat_g_disc = gaussian_discriminant.predict(x_test)\n",
    "    #print(yhat_g_disc)\n",
    "    \n",
    "    n_per_class = np.zeros((one_hot_y_test.shape[1], 1))\n",
    "    for line in one_hot_y_test:\n",
    "        for i in range(0, len(line)):\n",
    "            if line[i] == 1:\n",
    "                n_per_class[i] += 1\n",
    "                break\n",
    "    \n",
    "    for i, clss in enumerate(one_hot_y_test):\n",
    "        if np.array_equal(clss, yhat_softmax[i]):\n",
    "            if clss[0] == 1:\n",
    "                acuracias[0, f, 0] += 1\n",
    "            elif clss[1] == 1:\n",
    "                acuracias[0, f, 1] += 1\n",
    "            elif clss[2] == 1:\n",
    "                acuracias[0, f, 2] += 1\n",
    "            else:\n",
    "                acuracias[0, f, 3] += 1\n",
    "        \n",
    "        if np.array_equal(clss, yhat_gnb[i]):\n",
    "            if clss[0] == 1:\n",
    "                acuracias[1, f, 0] += 1\n",
    "            elif clss[1] == 1:\n",
    "                acuracias[1, f, 1] += 1\n",
    "            elif clss[2] == 1:\n",
    "                acuracias[1, f, 2] += 1\n",
    "            else:\n",
    "                acuracias[1, f, 3] += 1\n",
    "                \n",
    "        if np.array_equal(clss, yhat_g_disc[i]):\n",
    "            if clss[0] == 1:\n",
    "                acuracias[2, f, 0] += 1\n",
    "            elif clss[1] == 1:\n",
    "                acuracias[2, f, 1] += 1\n",
    "            elif clss[2] == 1:\n",
    "                acuracias[2, f, 2] += 1\n",
    "            else:\n",
    "                acuracias[2, f, 3] += 1\n",
    "    \n",
    "    acuracias[0, f, 0] /= n_per_class[0]\n",
    "    acuracias[0, f, 1] /= n_per_class[1]\n",
    "    acuracias[0, f, 2] /= n_per_class[2]\n",
    "    acuracias[0, f, 3] /= n_per_class[3]\n",
    "    acuracias[1, f, 0] /= n_per_class[0]\n",
    "    acuracias[1, f, 1] /= n_per_class[1]\n",
    "    acuracias[1, f, 2] /= n_per_class[2]\n",
    "    acuracias[1, f, 3] /= n_per_class[3]\n",
    "    acuracias[2, f, 0] /= n_per_class[0]\n",
    "    acuracias[2, f, 1] /= n_per_class[1]\n",
    "    acuracias[2, f, 2] /= n_per_class[2]\n",
    "    acuracias[2, f, 3] /= n_per_class[3]\n",
    "    \n",
    "#print(acuracias)\n",
    "\n",
    "print(\"Análise da Regressão Softmax(GD):\\n\")\n",
    "print(\"Média de acurácia:                 \", acuracias[0].mean())\n",
    "print(\"Desvio padrão da acurácia:         \", acuracias[0].std())\n",
    "print(\"Média da classe positiva:          \", acuracias[0, :, 0].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[0, :, 0].std())\n",
    "print(\"Média da classe Negativa:          \", acuracias[0, :, 1].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[0, :, 1].std())\n",
    "\n",
    "print(\"\\n--------------------------------------------------\\n\")\n",
    "\n",
    "print(\"Análise do Naive Bayes:\\n\")\n",
    "print(\"Média de acurácia:                 \", acuracias[1].mean())\n",
    "print(\"Desvio padrão da acurácia:         \", acuracias[1].std())\n",
    "print(\"Média da classe positiva:          \", acuracias[1, :, 0].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[1, :, 0].std())\n",
    "print(\"Média da classe Negativa:          \", acuracias[1, :, 1].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[1, :, 1].std())\n",
    "\n",
    "print(\"\\n--------------------------------------------------\\n\")\n",
    "\n",
    "print(\"Análise do Discriminante Gaussiano:\\n\")\n",
    "print(\"Média de acurácia:                 \", acuracias[2].mean())\n",
    "print(\"Variância da acurácia:             \", acuracias[2].var())\n",
    "print(\"Média da classe positiva:          \", acuracias[2, :, 0].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[2, :, 0].std())\n",
    "print(\"Média da classe Negativa:          \", acuracias[2, :, 1].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[2, :, 1].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f54dec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
