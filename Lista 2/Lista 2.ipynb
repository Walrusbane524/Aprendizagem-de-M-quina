{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5b2dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba31ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zScoreScale(x):\n",
    "    data = x\n",
    "    mean = data.mean()\n",
    "    sig = np.sqrt(((data - mean)**2).sum())\n",
    "    scaled_x = np.zeros(x.shape)\n",
    "    \n",
    "    scaled_data = (data - mean)/sig\n",
    "    for i in range(x.shape[1]):\n",
    "        scaled_x[:,i] = scaled_data[:,i]\n",
    "    return scaled_x, mean, sig\n",
    "\n",
    "def zScoreDescale(x, mean, sig):\n",
    "    scaled_data = x\n",
    "    descaled_data = scaled_data * sig + mean\n",
    "    \n",
    "    descaled_x = np.zeros(x.shape)\n",
    "    for i in range(x.shape[1]):\n",
    "        descaled_x[:,i] = descaled_data[:,i]\n",
    "        \n",
    "    return descaled_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c1757a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticalRegression:\n",
    "    \n",
    "    # Construtor\n",
    "    def __init__(self, x, y, addOnes = True):\n",
    "        if addOnes:\n",
    "            self.x = np.c_[np.ones(x.shape[0]), x]\n",
    "        else:\n",
    "            self.x = x\n",
    "        self.y = y\n",
    "        self.w = np.zeros(self.x.shape[1]).reshape(-1,1)\n",
    "        self.MSE = 0.0\n",
    "    \n",
    "    # Getters\n",
    "    def getX(self):\n",
    "        return self.x\n",
    "    \n",
    "    def getY(self):\n",
    "        return self.y\n",
    "    \n",
    "    def getW(self):\n",
    "        return self.w\n",
    "    \n",
    "    def getMSE(self):\n",
    "        return self.MSE\n",
    "    \n",
    "    # Setters\n",
    "    def setX(self, x, addOnes = True):\n",
    "        if addOnes:\n",
    "            x = np.c_[np.ones(x.shape[0]), x]\n",
    "        self.x = x\n",
    "        self.w = np.zeros(self.x.shape[1]).reshape(-1,1)\n",
    "    \n",
    "    def setY(self, y):\n",
    "        self.y = y\n",
    "    \n",
    "    # Métodos\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def trainGD(self, alpha = 0.1, max_iterations = 100):\n",
    "        \n",
    "        self.w = np.zeros(self.x.shape[1]).reshape(-1,1)\n",
    "        n = len(self.y)                 \n",
    "        for t in range(max_iterations):\n",
    "            yhat = np.zeros(n).reshape(-1,1)\n",
    "            e = np.zeros(n).reshape(-1,1)\n",
    "            \n",
    "            yhat = self.sigmoid(self.x @ self.w)\n",
    "            e = self.y - yhat\n",
    "            self.w[0] = self.w[0] + (alpha/n * e.sum())\n",
    "            for column in range(1, len(self.w)):\n",
    "                self.w[column] = self.w[column] + alpha/n * ((e * self.x[:,[column]]).sum())\n",
    "                    \n",
    "        self.MSE = ((e ** 2).sum())/(2*n)\n",
    "    \n",
    "    def test(self):\n",
    "        return self.y - (self.x @ self.w)\n",
    "    \n",
    "    def predict(self, x, addOnes = True):\n",
    "        if addOnes:\n",
    "            x = np.c_[np.ones(x.shape[0]), x]\n",
    "        return np.sign(x @ self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3140bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassLogisticalRegression:\n",
    "    \n",
    "    # Construtor\n",
    "    def __init__(self, x, y, addOnes = True):\n",
    "        if addOnes:\n",
    "            self.x = np.c_[np.ones(x.shape[0]), x]\n",
    "        else:\n",
    "            self.x = x\n",
    "        self.y = y\n",
    "        self.w = np.zeros((self.x.shape[1], y.shape[1]))\n",
    "        self.MSE = 0.0\n",
    "    \n",
    "    # Getters\n",
    "    def getX(self):\n",
    "        return self.x\n",
    "    \n",
    "    def getY(self):\n",
    "        return self.y\n",
    "    \n",
    "    def getW(self):\n",
    "        return self.w\n",
    "    \n",
    "    def getMSE(self):\n",
    "        return self.MSE\n",
    "    \n",
    "    # Setters\n",
    "    def setXY(self, x, y, addOnes = True):\n",
    "        if addOnes:\n",
    "            x = np.c_[np.ones(x.shape[0]), x]\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.w = np.zeros((self.x.shape[1], y.shape[1]))\n",
    "    \n",
    "    # Métodos\n",
    "    def softmax(self, w, x):\n",
    "        numerator = np.exp(x @ w)\n",
    "        denominator = np.sum(np.exp(x @ w), axis=1).reshape(-1,1)\n",
    "        return numerator/denominator\n",
    "    \n",
    "    def trainGD(self, alpha = 0.1, max_iterations = 100):\n",
    "    \n",
    "        n = self.y.shape[0]\n",
    "        yhat = np.zeros(self.y.shape)\n",
    "        e = np.zeros(self.y.shape)\n",
    "                 \n",
    "        for t in range(max_iterations):\n",
    "            yhat = self.softmax(self.w, self.x)\n",
    "            e = self.y - yhat\n",
    "            self.w = self.w + (alpha/n * (self.x.T @ e))\n",
    "        \n",
    "        print(self.w)\n",
    "        self.MSE = ((e ** 2).sum())/(2*n)\n",
    "    \n",
    "    def test(self):\n",
    "        return self.y - (self.x @ self.w)\n",
    "    \n",
    "    def predict(self, x, addOnes = True):\n",
    "        if addOnes:\n",
    "            x = np.c_[np.ones(x.shape[0]), x]\n",
    "        probabilities = self.softmax(self.w, x)\n",
    "        max_indexes = np.argmax(probabilities, axis=1)\n",
    "        prediction = np.zeros(probabilities.shape)\n",
    "        prediction[np.arange(len(max_indexes)), max_indexes] = 1\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "769ae41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesGaussiano:\n",
    "    \n",
    "    # Construtor \n",
    "    def __init__(self, x, y):\n",
    "        self.classes = np.eye(y.shape[1])\n",
    "        self.n_per_class = np.zeros((y.shape[1], 1))\n",
    "        self.class_priors = np.zeros((y.shape[1], 1)) # Probabilidade de cada classe\n",
    "        self.means = np.zeros((self.classes.shape[1], x.shape[1]))\n",
    "        self.variances = np.zeros((self.classes.shape[1], x.shape[1], x.shape[1]))\n",
    "        \n",
    "        # Calcula n_per_class\n",
    "        for line in y:\n",
    "            for i in range(0, len(line)):\n",
    "                if line[i] == 1:\n",
    "                    self.n_per_class[i] += 1\n",
    "                    break\n",
    "        \n",
    "        # Calcula class_priors\n",
    "        for i in range(0, len(self.class_priors)):\n",
    "            self.class_priors[i] = self.n_per_class[i]/self.n_per_class.sum()\n",
    "        \n",
    "        # Calcula a média de cada atributo para cada classe\n",
    "        for i in range(0, y.shape[0]):\n",
    "            for c in range(self.classes.shape[0]):\n",
    "                if np.array_equal(y[i], self.classes[c]):\n",
    "                    self.means[c] += x[i]\n",
    "                    break      \n",
    "        for clss in range(0, y.shape[1]):\n",
    "            self.means[clss, :] = self.means[clss, :]/self.n_per_class[clss]\n",
    "    \n",
    "        # Calcula a variância de cada feature\n",
    "        for clss in range(self.classes.shape[0]):\n",
    "            for feature in range(x.shape[1]):\n",
    "                self.variances[clss, feature, feature] = np.sum((x[y[:, clss] == 1, feature] - self.means[clss, feature])**2)/(self.n_per_class[clss] - 1)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        prob = np.zeros((self.classes.shape[0], 1))\n",
    "        for clss in range(self.classes.shape[0]):\n",
    "            prob[clss] = np.log(self.class_priors[clss])\n",
    "            prob[clss] -= 0.5 * np.sum(np.log(2 * np.pi * self.variances[clss, :].sum()))\n",
    "            prob[clss] -= 0.5 * np.sum(((x - self.means[clss, :])**2)/self.variances[clss, :].sum())\n",
    "        return self.classes[np.argmax(prob)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f49cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminanteGaussiano:\n",
    "    \n",
    "    # Construtor \n",
    "    def __init__(self, x, y):\n",
    "        self.classes = np.eye(y.shape[1])\n",
    "        self.n_per_class = np.zeros((y.shape[1], 1))\n",
    "        self.class_priors = np.zeros((y.shape[1], 1)) # Probabilidade de cada classe\n",
    "        self.means = np.zeros((self.classes.shape[1], x.shape[1]))\n",
    "        self.variances = np.zeros((self.classes.shape[1], x.shape[1], x.shape[1]))\n",
    "        \n",
    "        # Calcula n_per_class\n",
    "        for line in y:\n",
    "            for i in range(0, len(line)):\n",
    "                if line[i] == 1:\n",
    "                    self.n_per_class[i] += 1\n",
    "                    break\n",
    "        \n",
    "        # Calcula class_priors\n",
    "        for i in range(0, len(self.class_priors)):\n",
    "            self.class_priors[i] = self.n_per_class[i]/self.n_per_class.sum()\n",
    "        \n",
    "        # Calcula a média de cada atributo para cada classe\n",
    "        for i in range(0, y.shape[0]):\n",
    "            for c in range(self.classes.shape[0]):\n",
    "                if np.array_equal(y[i], self.classes[c]):\n",
    "                    self.means[c] += x[i]\n",
    "                    break      \n",
    "        for clss in range(0, y.shape[1]):\n",
    "            self.means[clss, :] = self.means[clss, :]/self.n_per_class[clss]\n",
    "    \n",
    "        # Calcula a variância de cada feature\n",
    "        \n",
    "        for clss in range(self.classes.shape[0]):\n",
    "            self.variances[clss] = np.cov(x[y[:, clss] == 1, :], rowvar=False)/self.n_per_class[clss]\n",
    "            np.linalg.inv(self.variances[clss])\n",
    "            \n",
    "    def predict(self, x):\n",
    "        prob = np.zeros((self.classes.shape[0], 1))\n",
    "        for clss in range(self.classes.shape[0]):\n",
    "            \n",
    "            det = np.linalg.det(self.variances[clss])\n",
    "            inv = np.linalg.inv(self.variances[clss])\n",
    "            \n",
    "            prob[clss] = np.log(self.class_priors[clss])\n",
    "            prob[clss] -= 0.5 * np.log(det)\n",
    "            prob[clss] -= 0.5 * (x - self.means[clss, :]).T @ inv @ (x - self.means[clss, :])\n",
    "        return self.classes[np.argmax(prob)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df3e69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold(data):\n",
    "    folds = []\n",
    "    splits = 10\n",
    "    indices = np.random.permutation(data.shape[0])\n",
    "    folds_idx = np.array_split(indices, splits)\n",
    "    \n",
    "    for i in range(splits):\n",
    "        train_idx = np.concatenate(folds_idx[:i] + folds_idx[i+1:])\n",
    "        test_idx = folds_idx[i]\n",
    "        folds.append((train_idx, test_idx))\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00de6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(\"breastcancer.csv\", delimiter=',')\n",
    "\n",
    "x = data[:, :30]\n",
    "y = data[:, 30]\n",
    "\n",
    "scaled_x, mean, sig = zScoreScale(x)\n",
    "\n",
    "one_hot_y = np.zeros((x.shape[0], 2))\n",
    "for l in range(len(y)):\n",
    "    if y[l] == 1:\n",
    "        one_hot_y[l] = np.array([1, 0])\n",
    "    else:\n",
    "        one_hot_y[l] = np.array([0, 1])\n",
    "\n",
    "folds = kfold(x)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(folds):\n",
    "    x_train = scaled_x[train_idx, :]\n",
    "    y_train = y[train_idx]\n",
    "    one_hot_y_train = one_hot_y[train_idx]\n",
    "    \n",
    "    x_test = scaled_x[test_idx, :]\n",
    "    y_test = y[test_idx]\n",
    "    one_hot_y_test = one_hot_y[test_idx]\n",
    "    \n",
    "    logistical_gd = BinaryLogisticalRegression(x_train, y_train)\n",
    "    gaussian_discriminant = DiscriminanteGaussiano(x_train, one_hot_y_train)\n",
    "    gaussian_naive_bayes = NaiveBayesGaussiano(x_train, one_hot_y_train)\n",
    "    \n",
    "    logistical_gd.trainGD(max_iterations = 200)\n",
    "    yhat_lgd = logistical_gd.predict(x_test)\n",
    "    print(yhat_lgd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012903f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
