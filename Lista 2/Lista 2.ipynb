{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "e5b2dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "6ba31ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zScoreScale(x):\n",
    "    data = x\n",
    "    mean = data.mean()\n",
    "    sig = np.sqrt(((data - mean)**2).sum())\n",
    "    scaled_x = np.zeros(x.shape)\n",
    "    \n",
    "    scaled_data = (data - mean)/sig\n",
    "    for i in range(x.shape[1]):\n",
    "        scaled_x[:,i] = scaled_data[:,i]\n",
    "    return scaled_x, mean, sig\n",
    "\n",
    "def zScoreDescale(x, mean, sig):\n",
    "    scaled_data = x\n",
    "    descaled_data = scaled_data * sig + mean\n",
    "    \n",
    "    descaled_x = np.zeros(x.shape)\n",
    "    for i in range(x.shape[1]):\n",
    "        descaled_x[:,i] = descaled_data[:,i]\n",
    "        \n",
    "    return descaled_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "6c1757a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticalRegression:\n",
    "    \n",
    "    # Construtor\n",
    "    def __init__(self, x, y, addOnes = True):\n",
    "        if addOnes:\n",
    "            self.x = np.c_[np.ones(x.shape[0]), x]\n",
    "        else:\n",
    "            self.x = x\n",
    "        self.y = y\n",
    "        self.w = np.zeros(self.x.shape[1]).reshape(-1,1)\n",
    "        self.MSE = 0.0\n",
    "    \n",
    "    # Getters\n",
    "    def getX(self):\n",
    "        return self.x\n",
    "    \n",
    "    def getY(self):\n",
    "        return self.y\n",
    "    \n",
    "    def getW(self):\n",
    "        return self.w\n",
    "    \n",
    "    def getMSE(self):\n",
    "        return self.MSE\n",
    "    \n",
    "    # Setters\n",
    "    def setX(self, x, addOnes = True):\n",
    "        if addOnes:\n",
    "            x = np.c_[np.ones(x.shape[0]), x]\n",
    "        self.x = x\n",
    "        self.w = np.zeros(self.x.shape[1]).reshape(-1,1)\n",
    "    \n",
    "    def setY(self, y):\n",
    "        self.y = y\n",
    "    \n",
    "    # Métodos\n",
    "    def sigmoid(self, z):\n",
    "        return (1/(1 + np.exp(-1 * z)))\n",
    "    \n",
    "    def trainGD(self, alpha = 0.1, l2 = 0.01, max_iterations = 100):\n",
    "        \n",
    "        n = len(self.y)\n",
    "        yhat = np.zeros(n).reshape(-1,1)\n",
    "        e = np.zeros(n).reshape(-1,1)\n",
    "        \n",
    "        for t in range(max_iterations):\n",
    "            \n",
    "            yhat = self.sigmoid(self.x @ self.w)\n",
    "            e = self.y - yhat\n",
    "            \n",
    "            self.w = self.w + (alpha/n) * self.x.T @ e  \n",
    "        \n",
    "        self.MSE = ((e ** 2).sum())/(2*n)\n",
    "    \n",
    "    def test(self):\n",
    "        return self.y - (self.x @ self.w)\n",
    "    \n",
    "    def predict(self, x, addOnes = True):\n",
    "        if addOnes:\n",
    "            x = np.c_[np.ones(x.shape[0]), x]\n",
    "        y_pred = self.sigmoid(x @ self.w)\n",
    "        y_pred[y_pred > 0.5] = 1\n",
    "        y_pred[y_pred <= 0.5] = 0\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "af3140bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassLogisticalRegression:\n",
    "    \n",
    "    # Construtor\n",
    "    def __init__(self, x, y, addOnes = True):\n",
    "        if addOnes:\n",
    "            self.x = np.c_[np.ones(x.shape[0]), x]\n",
    "        else:\n",
    "            self.x = x\n",
    "        self.y = y\n",
    "        self.w = np.zeros((self.x.shape[1], y.shape[1]))\n",
    "    \n",
    "    # Getters\n",
    "    def getX(self):\n",
    "        return self.x\n",
    "    \n",
    "    def getY(self):\n",
    "        return self.y\n",
    "    \n",
    "    def getW(self):\n",
    "        return self.w\n",
    "    \n",
    "    def getMSE(self):\n",
    "        return self.MSE\n",
    "    \n",
    "    # Setters\n",
    "    def setXY(self, x, y, addOnes = True):\n",
    "        if addOnes:\n",
    "            x = np.c_[np.ones(x.shape[0]), x]\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.w = np.zeros((self.x.shape[1], y.shape[1]))\n",
    "    \n",
    "    # Métodos\n",
    "    def softmax(self, w, x):\n",
    "        numerator = np.exp(x @ w)\n",
    "        denominator = np.sum(np.exp(x @ w), axis=1).reshape(-1,1)\n",
    "        return numerator/denominator\n",
    "    \n",
    "    def trainGD(self, alpha = 0.1, max_iterations = 100):\n",
    "    \n",
    "        n = self.y.shape[0]\n",
    "        yhat = np.zeros(self.y.shape)\n",
    "        e = np.zeros(self.y.shape)\n",
    "                 \n",
    "        for t in range(max_iterations):\n",
    "            yhat = self.softmax(self.w, self.x)\n",
    "            e = self.y - yhat\n",
    "            \n",
    "            self.w = self.w + (alpha/n) * self.x.T @ e\n",
    "    \n",
    "    def test(self):\n",
    "        return self.y - (self.x @ self.w)\n",
    "    \n",
    "    def predict(self, x, addOnes = True):\n",
    "        if addOnes:\n",
    "            x = np.c_[np.ones(x.shape[0]), x]\n",
    "            \n",
    "        probabilities = self.softmax(self.w, x)\n",
    "        max_indexes = np.argmax(probabilities, axis=1)\n",
    "        \n",
    "        prediction = np.zeros(probabilities.shape)\n",
    "        prediction[np.arange(len(max_indexes)), max_indexes] = 1\n",
    "        \n",
    "        #print(prediction)\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "769ae41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesGaussiano:\n",
    "    \n",
    "    # Construtor \n",
    "    def __init__(self, x, y):\n",
    "        self.classes = np.eye(y.shape[1])\n",
    "        self.n_per_class = np.zeros((y.shape[1], 1))\n",
    "        self.class_priors = np.zeros((y.shape[1], 1)) # Probabilidade de cada classe\n",
    "        self.means = np.zeros((self.classes.shape[1], x.shape[1]))\n",
    "        self.variances = np.zeros((self.classes.shape[1], x.shape[1], x.shape[1]))\n",
    "        \n",
    "        # Calcula n_per_class\n",
    "        for line in y:\n",
    "            for i in range(0, len(line)):\n",
    "                if line[i] == 1:\n",
    "                    self.n_per_class[i] += 1\n",
    "                    break\n",
    "        \n",
    "        # Calcula class_priors\n",
    "        for i in range(0, len(self.class_priors)):\n",
    "            self.class_priors[i] = self.n_per_class[i]/self.n_per_class.sum()\n",
    "        \n",
    "        # Calcula a média de cada atributo para cada classe\n",
    "        for i in range(0, y.shape[0]):\n",
    "            for c in range(self.classes.shape[0]):\n",
    "                if np.array_equal(y[i], self.classes[c]):\n",
    "                    self.means[c] += x[i]\n",
    "                    break      \n",
    "        for clss in range(0, y.shape[1]):\n",
    "            self.means[clss, :] = self.means[clss, :]/self.n_per_class[clss]\n",
    "    \n",
    "        # Calcula a variância de cada feature\n",
    "        for clss in range(self.classes.shape[0]):\n",
    "            for feature in range(x.shape[1]):\n",
    "                self.variances[clss, feature, feature] = np.sum((x[y[:, clss] == 1, feature] - self.means[clss, feature])**2)/(self.n_per_class[clss] - 1)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        probs = np.zeros((x.shape[0], self.classes.shape[0]))\n",
    "        y_pred = np.zeros(probs.shape)\n",
    "        for i in range(x.shape[0]):\n",
    "            for clss in range(self.classes.shape[0]):\n",
    "                probs[i, clss] = np.log(self.class_priors[clss])\n",
    "                probs[i, clss] -= 0.5 * np.sum(np.log(2 * np.pi * self.variances[clss, :].sum()))\n",
    "                probs[i, clss] -= 0.5 * np.sum(((x[i] - self.means[clss, :])**2)/self.variances[clss, :].sum())\n",
    "        for i in range(x.shape[0]):\n",
    "            y_pred[i] = self.classes[np.argmax(probs[i])]\n",
    "    \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "4f49cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminanteGaussiano:\n",
    "    \n",
    "    # Construtor \n",
    "    def __init__(self, x, y):\n",
    "        self.classes = np.eye(y.shape[1])\n",
    "        self.n_per_class = np.zeros((y.shape[1], 1))\n",
    "        self.class_priors = np.zeros((y.shape[1], 1)) # Probabilidade de cada classe\n",
    "        self.means = np.zeros((self.classes.shape[1], x.shape[1]))\n",
    "        self.variances = np.zeros((self.classes.shape[1], x.shape[1], x.shape[1]))\n",
    "        \n",
    "        # Calcula n_per_class\n",
    "        for line in y:\n",
    "            for i in range(0, len(line)):\n",
    "                if line[i] == 1:\n",
    "                    self.n_per_class[i] += 1\n",
    "                    break\n",
    "        \n",
    "        # Calcula class_priors\n",
    "        for i in range(0, len(self.class_priors)):\n",
    "            self.class_priors[i] = self.n_per_class[i]/self.n_per_class.sum()\n",
    "        \n",
    "        # Calcula a média de cada atributo para cada classe\n",
    "        for i in range(0, y.shape[0]):\n",
    "            for c in range(self.classes.shape[0]):\n",
    "                if np.array_equal(y[i], self.classes[c]):\n",
    "                    self.means[c] += x[i]\n",
    "                    break      \n",
    "        for clss in range(0, y.shape[1]):\n",
    "            self.means[clss, :] = self.means[clss, :]/self.n_per_class[clss]\n",
    "    \n",
    "        \n",
    "        # Calcula a variância de cada feature para cada classe\n",
    "        for clss in range(self.classes.shape[0]):\n",
    "            self.variances[clss] = np.cov(x[y[:, clss] == 1, :], rowvar=False)/self.n_per_class[clss]\n",
    "            self.variances[clss] += np.identity(self.variances.shape[1]) * 1e-10\n",
    "            \n",
    "    def predict(self, x):\n",
    "        probs = np.zeros((x.shape[0], self.classes.shape[0]))\n",
    "        y_pred = np.zeros(probs.shape)\n",
    "        for i in range(x.shape[0]):\n",
    "            for clss in range(self.classes.shape[0]):\n",
    "            \n",
    "                det = np.linalg.det(self.variances[clss])\n",
    "                inv = np.linalg.inv(self.variances[clss])\n",
    "            \n",
    "                probs[i, clss] = np.log(self.class_priors[clss])\n",
    "                probs[i, clss] -= 0.5 * np.log(det)\n",
    "                probs[i, clss] -= 0.5 * (x[i] - self.means[clss, :]) @ inv @ (x[i] - self.means[clss, :]).T\n",
    "                \n",
    "        for i in range(x.shape[0]):\n",
    "            y_pred[i] = self.classes[np.argmax(probs[i])]\n",
    "            \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "df3e69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold(data):\n",
    "    np.random.seed(1200)\n",
    "    folds = []\n",
    "    splits = 10\n",
    "    indices = np.random.permutation(data.shape[0])\n",
    "    folds_idx = np.array_split(indices, splits)\n",
    "        \n",
    "    for i in range(0, splits):\n",
    "        train_idx = np.concatenate(folds_idx[:i] + folds_idx[i+1:])\n",
    "        test_idx = folds_idx[i]\n",
    "        folds.append((train_idx, test_idx))\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77774b",
   "metadata": {},
   "source": [
    "# Questão 1\n",
    "\n",
    "Professor, quando fiz meu algoritmo de regressão logística binária usando GD, testei num dataset pequeno e ele funcionou sem problemas. Porém, quando testo no dataset da questão, o resultado da predição inteira é de uma classe só, com pesos bem similares. Passei horas analisando o código e não consegui encontrar o problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "a00de6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análise da Regressão Logística(GD):\n",
      "\n",
      "Média de acurácia:                  0.5\n",
      "Desvio padrão da acurácia:          0.5\n",
      "Média da classe positiva:           0.0\n",
      "Desvio padrão da classe positiva:   0.0\n",
      "Média da classe Negativa:           1.0\n",
      "Desvio padrão da classe positiva:   0.0\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Análise do Naive Bayes:\n",
      "\n",
      "Média de acurácia:                  0.8849795249573799\n",
      "Desvio padrão da acurácia:          0.1103959596017686\n",
      "Média da classe positiva:           0.7926406316795332\n",
      "Desvio padrão da classe positiva:   0.081765808487049\n",
      "Média da classe Negativa:           0.9773184182352264\n",
      "Desvio padrão da classe positiva:   0.02521796842917371\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Análise do discriminante gaussiano:\n",
      "\n",
      "Média de acurácia:                  0.8581028003311534\n",
      "Variância da acurácia:              0.0220137644003484\n",
      "Média da classe positiva:           1.0\n",
      "Desvio padrão da classe positiva:   0.0\n",
      "Média da classe Negativa:           0.716205600662307\n",
      "Desvio padrão da classe positiva:   0.06130169861411506\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt(\"breastcancer.csv\", delimiter=',')\n",
    "\n",
    "x = data[:, :30]\n",
    "y = data[:, 30].reshape(-1, 1)\n",
    "\n",
    "scaled_x, mean, sig = zScoreScale(x)\n",
    "\n",
    "one_hot_y = np.zeros((x.shape[0], 2))\n",
    "for l in range(len(y)):\n",
    "    if y[l] == 1:\n",
    "        one_hot_y[l] = np.array([1, 0])\n",
    "    else:\n",
    "        one_hot_y[l] = np.array([0, 1])\n",
    "\n",
    "folds = kfold(x)\n",
    "acuracias = np.zeros((3, 10, 2))\n",
    "\n",
    "for f, (train_idx, test_idx) in enumerate(folds):\n",
    "#(train_idx, test_idx) = folds[0]\n",
    "\n",
    "    x_train = scaled_x[train_idx, :]\n",
    "    y_train = y[train_idx].reshape(-1,1)\n",
    "    one_hot_y_train = one_hot_y[train_idx]\n",
    "\n",
    "    x_test = scaled_x[test_idx, :]\n",
    "    y_test = y[test_idx].reshape(-1,1)\n",
    "    one_hot_y_test = one_hot_y[test_idx]\n",
    "\n",
    "    logistical_gd = BinaryLogisticalRegression(x_train, y_train)\n",
    "    gaussian_discriminant = DiscriminanteGaussiano(x_train, one_hot_y_train)\n",
    "    gaussian_naive_bayes = NaiveBayesGaussiano(x_train, one_hot_y_train)\n",
    "\n",
    "    logistical_gd.trainGD(alpha = 0.1, max_iterations = 1000)\n",
    "    yhat_lgd = logistical_gd.predict(x_test)\n",
    "    #print(logistical_gd.getW())\n",
    "    #print(yhat_lgd)\n",
    "\n",
    "    yhat_gnb = gaussian_naive_bayes.predict(x_test)\n",
    "    yhat_g_disc = gaussian_discriminant.predict(x_test)\n",
    "    \n",
    "    \n",
    "    n_per_class = np.zeros((one_hot_y_test.shape[1], 1))\n",
    "    for line in one_hot_y_test:\n",
    "        for i in range(0, len(line)):\n",
    "            if line[i] == 1:\n",
    "                n_per_class[i] += 1\n",
    "                break\n",
    "    \n",
    "    for i, clss in enumerate(one_hot_y_test):\n",
    "        if ((yhat_lgd[i] == 0) and (clss[1] == 1)) or ((yhat_lgd[i] == 1) and (clss[0] == 1)):\n",
    "            if clss[0] == 1:\n",
    "                acuracias[0, f, 0] += 1\n",
    "            else:\n",
    "                acuracias[0, f, 1] += 1\n",
    "        \n",
    "        if np.array_equal(clss, yhat_gnb[i]):\n",
    "            if clss[0] == 1:\n",
    "                acuracias[1, f, 0] += 1\n",
    "            else:\n",
    "                acuracias[1, f, 1] += 1\n",
    "                \n",
    "        if np.array_equal(clss, yhat_g_disc[i]):\n",
    "            if clss[0] == 1:\n",
    "                acuracias[2, f, 0] += 1\n",
    "            else:\n",
    "                acuracias[2, f, 1] += 1\n",
    "    \n",
    "    acuracias[0, f, 0] /= n_per_class[0]\n",
    "    acuracias[0, f, 1] /= n_per_class[1]\n",
    "    acuracias[1, f, 0] /= n_per_class[0]\n",
    "    acuracias[1, f, 1] /= n_per_class[1]\n",
    "    acuracias[2, f, 0] /= n_per_class[0]\n",
    "    acuracias[2, f, 1] /= n_per_class[1]\n",
    "\n",
    "#print(acuracias)\n",
    "\n",
    "print(\"Análise da Regressão Logística(GD):\\n\")\n",
    "print(\"Média de acurácia:                 \", acuracias[0].mean())\n",
    "print(\"Desvio padrão da acurácia:         \", acuracias[0].std())\n",
    "print(\"Média da classe positiva:          \", acuracias[0, :, 0].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[0, :, 0].std())\n",
    "print(\"Média da classe Negativa:          \", acuracias[0, :, 1].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[0, :, 1].std())\n",
    "\n",
    "print(\"\\n--------------------------------------------------\\n\")\n",
    "\n",
    "print(\"Análise do Naive Bayes:\\n\")\n",
    "print(\"Média de acurácia:                 \", acuracias[1].mean())\n",
    "print(\"Desvio padrão da acurácia:         \", acuracias[1].std())\n",
    "print(\"Média da classe positiva:          \", acuracias[1, :, 0].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[1, :, 0].std())\n",
    "print(\"Média da classe Negativa:          \", acuracias[1, :, 1].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[1, :, 1].std())\n",
    "\n",
    "print(\"\\n--------------------------------------------------\\n\")\n",
    "\n",
    "print(\"Análise do discriminante gaussiano:\\n\")\n",
    "print(\"Média de acurácia:                 \", acuracias[2].mean())\n",
    "print(\"Variância da acurácia:             \", acuracias[2].var())\n",
    "print(\"Média da classe positiva:          \", acuracias[2, :, 0].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[2, :, 0].std())\n",
    "print(\"Média da classe Negativa:          \", acuracias[2, :, 1].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[2, :, 1].std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a87a1",
   "metadata": {},
   "source": [
    "# Questão 2\n",
    "\n",
    "O algoritmo de regressão softmax tem o mesmo problema do algoritmo de regressão binária. Fiz o algorimo e testei para um dataset pequeno. Porém, quando chamo o método predict as saídas temdem a ser de uma classe só. Também passei horas tentando entender o que estava dando errado e falhei miseravelmente. Novamente, peço perdão professor, eu tentei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "c95dbb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análise da Regressão Softmax(GD):\n",
      "\n",
      "Média de acurácia:                  0.2713311688311688\n",
      "Desvio padrão da acurácia:          0.4222434877571003\n",
      "Média da classe positiva:           0.5253246753246753\n",
      "Desvio padrão da classe positiva:   0.45074898782504147\n",
      "Média da classe Negativa:           0.175\n",
      "Desvio padrão da classe positiva:   0.35443617196894556\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Análise do Naive Bayes:\n",
      "\n",
      "Média de acurácia:                  0.4127574385474486\n",
      "Desvio padrão da acurácia:          0.3585228258506487\n",
      "Média da classe positiva:           0.01814194577352472\n",
      "Desvio padrão da classe positiva:   0.022556552796702017\n",
      "Média da classe Negativa:           0.3857632549511591\n",
      "Desvio padrão da classe positiva:   0.08285542540852096\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Análise do Discriminante Gaussiano:\n",
      "\n",
      "Média de acurácia:                  0.7897849938249699\n",
      "Variância da acurácia:              0.05715240118837599\n",
      "Média da classe positiva:           0.9633673831042252\n",
      "Desvio padrão da classe positiva:   0.04773088883311959\n",
      "Média da classe Negativa:           0.7831841055189328\n",
      "Desvio padrão da classe positiva:   0.10612320150174107\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt(\"vehicle.csv\", delimiter=',')\n",
    "\n",
    "x = data[:, :18]\n",
    "y = data[:, 18].reshape(-1, 1)\n",
    "\n",
    "scaled_x, mean, sig = zScoreScale(x)\n",
    "\n",
    "one_hot_y = np.zeros((x.shape[0], 4))\n",
    "for l in range(len(y)):\n",
    "    match(y[l]):\n",
    "        case 0:\n",
    "            one_hot_y[l] = np.array([1, 0, 0, 0])\n",
    "        case 1:\n",
    "            one_hot_y[l] = np.array([0, 1, 0, 0])\n",
    "        case 2:\n",
    "            one_hot_y[l] = np.array([0, 0, 1, 0])\n",
    "        case 3:\n",
    "            one_hot_y[l] = np.array([0, 0, 0, 1])\n",
    "\n",
    "folds = kfold(x)\n",
    "acuracias = np.zeros((3, 10, 4))\n",
    "\n",
    "for f, (train_idx, test_idx) in enumerate(folds):\n",
    "#(train_idx, test_idx) = folds[0]\n",
    "\n",
    "    x_train = scaled_x[train_idx, :]\n",
    "    y_train = y[train_idx].reshape(-1,1)\n",
    "    one_hot_y_train = one_hot_y[train_idx]\n",
    "\n",
    "    x_test = scaled_x[test_idx, :]\n",
    "    y_test = y[test_idx].reshape(-1,1)\n",
    "    one_hot_y_test = one_hot_y[test_idx]\n",
    "\n",
    "    softmax_gd = MulticlassLogisticalRegression(x_train, one_hot_y_train)\n",
    "    gaussian_discriminant = DiscriminanteGaussiano(x_train, one_hot_y_train)\n",
    "    gaussian_naive_bayes = NaiveBayesGaussiano(x_train, one_hot_y_train)\n",
    "\n",
    "    softmax_gd.trainGD(alpha = 0.1, max_iterations = 200)\n",
    "    yhat_softmax = softmax_gd.predict(x_test)\n",
    "    #print(logistical_gd.getW())\n",
    "    #print(yhat_lgd)\n",
    "\n",
    "    yhat_gnb = gaussian_naive_bayes.predict(x_test)\n",
    "    #print(yhat_gnb)\n",
    "    yhat_g_disc = gaussian_discriminant.predict(x_test)\n",
    "    #print(yhat_g_disc)\n",
    "    \n",
    "    n_per_class = np.zeros((one_hot_y_test.shape[1], 1))\n",
    "    for line in one_hot_y_test:\n",
    "        for i in range(0, len(line)):\n",
    "            if line[i] == 1:\n",
    "                n_per_class[i] += 1\n",
    "                break\n",
    "    \n",
    "    for i, clss in enumerate(one_hot_y_test):\n",
    "        if np.array_equal(clss, yhat_softmax[i]):\n",
    "            if clss[0] == 1:\n",
    "                acuracias[0, f, 0] += 1\n",
    "            elif clss[1] == 1:\n",
    "                acuracias[0, f, 1] += 1\n",
    "            elif clss[2] == 1:\n",
    "                acuracias[0, f, 2] += 1\n",
    "            else:\n",
    "                acuracias[0, f, 3] += 1\n",
    "        \n",
    "        if np.array_equal(clss, yhat_gnb[i]):\n",
    "            if clss[0] == 1:\n",
    "                acuracias[1, f, 0] += 1\n",
    "            elif clss[1] == 1:\n",
    "                acuracias[1, f, 1] += 1\n",
    "            elif clss[2] == 1:\n",
    "                acuracias[1, f, 2] += 1\n",
    "            else:\n",
    "                acuracias[1, f, 3] += 1\n",
    "                \n",
    "        if np.array_equal(clss, yhat_g_disc[i]):\n",
    "            if clss[0] == 1:\n",
    "                acuracias[2, f, 0] += 1\n",
    "            elif clss[1] == 1:\n",
    "                acuracias[2, f, 1] += 1\n",
    "            elif clss[2] == 1:\n",
    "                acuracias[2, f, 2] += 1\n",
    "            else:\n",
    "                acuracias[2, f, 3] += 1\n",
    "    \n",
    "    acuracias[0, f, 0] /= n_per_class[0]\n",
    "    acuracias[0, f, 1] /= n_per_class[1]\n",
    "    acuracias[0, f, 2] /= n_per_class[2]\n",
    "    acuracias[0, f, 3] /= n_per_class[3]\n",
    "    acuracias[1, f, 0] /= n_per_class[0]\n",
    "    acuracias[1, f, 1] /= n_per_class[1]\n",
    "    acuracias[1, f, 2] /= n_per_class[2]\n",
    "    acuracias[1, f, 3] /= n_per_class[3]\n",
    "    acuracias[2, f, 0] /= n_per_class[0]\n",
    "    acuracias[2, f, 1] /= n_per_class[1]\n",
    "    acuracias[2, f, 2] /= n_per_class[2]\n",
    "    acuracias[2, f, 3] /= n_per_class[3]\n",
    "    \n",
    "#print(acuracias)\n",
    "\n",
    "print(\"Análise da Regressão Softmax(GD):\\n\")\n",
    "print(\"Média de acurácia:                 \", acuracias[0].mean())\n",
    "print(\"Desvio padrão da acurácia:         \", acuracias[0].std())\n",
    "print(\"Média da classe positiva:          \", acuracias[0, :, 0].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[0, :, 0].std())\n",
    "print(\"Média da classe Negativa:          \", acuracias[0, :, 1].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[0, :, 1].std())\n",
    "\n",
    "print(\"\\n--------------------------------------------------\\n\")\n",
    "\n",
    "print(\"Análise do Naive Bayes:\\n\")\n",
    "print(\"Média de acurácia:                 \", acuracias[1].mean())\n",
    "print(\"Desvio padrão da acurácia:         \", acuracias[1].std())\n",
    "print(\"Média da classe positiva:          \", acuracias[1, :, 0].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[1, :, 0].std())\n",
    "print(\"Média da classe Negativa:          \", acuracias[1, :, 1].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[1, :, 1].std())\n",
    "\n",
    "print(\"\\n--------------------------------------------------\\n\")\n",
    "\n",
    "print(\"Análise do Discriminante Gaussiano:\\n\")\n",
    "print(\"Média de acurácia:                 \", acuracias[2].mean())\n",
    "print(\"Variância da acurácia:             \", acuracias[2].var())\n",
    "print(\"Média da classe positiva:          \", acuracias[2, :, 0].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[2, :, 0].std())\n",
    "print(\"Média da classe Negativa:          \", acuracias[2, :, 1].mean())\n",
    "print(\"Desvio padrão da classe positiva:  \", acuracias[2, :, 1].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0c95b3",
   "metadata": {},
   "source": [
    "## Exemplo das regressões logísticas funcionando para um dataset menor\n",
    "### Regressão logística binária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "18ddd8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticalRegression:\n",
    "    \n",
    "    # Construtor\n",
    "    def __init__(self, x, y, addOnes = True):\n",
    "        if addOnes:\n",
    "            self.x = np.c_[np.ones(x.shape[0]), x]\n",
    "        else:\n",
    "            self.x = x\n",
    "        self.y = y\n",
    "        self.w = np.zeros(self.x.shape[1]).reshape(-1,1)\n",
    "        self.MSE = 0.0\n",
    "    \n",
    "    # Getters\n",
    "    def getX(self):\n",
    "        return self.x\n",
    "    \n",
    "    def getY(self):\n",
    "        return self.y\n",
    "    \n",
    "    def getW(self):\n",
    "        return self.w\n",
    "    \n",
    "    def getMSE(self):\n",
    "        return self.MSE\n",
    "    \n",
    "    # Setters\n",
    "    def setX(self, x, addOnes = True):\n",
    "        if addOnes:\n",
    "            x = np.c_[np.ones(x.shape[0]), x]\n",
    "        self.x = x\n",
    "        self.w = np.zeros(self.x.shape[1]).reshape(-1,1)\n",
    "    \n",
    "    def setY(self, y):\n",
    "        self.y = y\n",
    "    \n",
    "    # Métodos\n",
    "    def logit(self, z):\n",
    "        return (1/(1 + np.exp(-1 * z)))\n",
    "    \n",
    "    def trainGD(self, alpha = 0.1, max_iterations = 100):\n",
    "    \n",
    "        n = len(self.y)\n",
    "        yhat = np.zeros(n).reshape(-1,1)\n",
    "        e = np.zeros(n).reshape(-1,1)\n",
    "                 \n",
    "        for t in range(max_iterations):\n",
    "            yhat = self.logit(self.x @ self.w)\n",
    "            e = self.y - yhat\n",
    "            \n",
    "            self.w = self.w + (alpha/n) * (self.x.T @ e)\n",
    "    \n",
    "    def test(self):\n",
    "        return self.y - (self.x @ self.w)\n",
    "    \n",
    "    def predict(self, x, addOnes = True):\n",
    "        if addOnes:\n",
    "            x = np.c_[np.ones(x.shape[0]), x]\n",
    "        y_pred = self.logit(x @ self.w)\n",
    "        y_pred[y_pred > 0.5] = 1\n",
    "        y_pred[y_pred <= 0.5] = 0\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "a3ba54ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAT70lEQVR4nO3df6zd9X3f8ecL4o44UKzNN8XxryttaGoJIVh3BsRSUciqQCBEHdKQnLJl0u5gqCNStEiNO1qmWt1fESJoeLdNu7LcJusUwoBBFRpAJeogu3YMDjVZrQyDAxoXVgyeURuT9/44X7fHl3t8z7WPfbkfPx/S0fl+P5/P+Z7311/5db/3c77nflNVSJKWvzOWugBJ0mgY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQpU6S/5TkN7vljyX5wZCvuz3Ji0kuTPL4ya1SGsxA17KT5IUkbyc5mOT/JPm9JGeP8j2q6smq+vtDDv8IcCXwJeDJUdYhLYaBruXquqo6G9gE/APg1/o7k7zvVBVSVTdU1d6q+kdVdfupel9pLgNdy1pV/Qh4BPhwkkpya5I/B/4cIMm1SXYleSPJnyb5yJHXJrk4yc4kbyX5L8BZfX1XJNnft74+yX1JZpO8nuTurv3vJnmsa3styXSSVX2v+9kkT3Tv/1yST530fxSdtgx0LWtJ1gPXAN/rmj4NXAL8XJJNwO8C/xL4O8B/BB5I8reS/BRwP/Cfgb8N/FfgHw94jzOBh4B9wDiwFvj6kW7gt4APAT8LrAd+o3vdCuBB4FvAB4FfAaaTDDuVIy1K/FsuWm6SvACsBg4DB4D/DnweOARcVVWPdePuAV6rqn/b99ofAJNA0QvltdX9J0jyp8BjVfVrSa4AvlpV65JcBjwArKmqwwvU9mng16vq4iQfo/eD4kNV9ZOu/2vAD6rqN0bwTyEd5ZTNM0oj9umq+uP+hiQAL/U1bQT+aZJf6Wv7KXpn0wX8qI4+o9k34L3WA/vmC/MkHwTuAj4GnEPvt96/6Lo/BLx0JMz73mPtsXdNOj5Ouag1/QH9ErCtqlb1PVZW1deAV4C16X4KdDYM2OZLwIYBH7T+VveeH6mqnwY+Q28aBuBlYH2S/v9nG4AfLX63pIUZ6GrZbwM3J7kkPR9I8skk5wD/g96Uzb9O8r4kvwRsHrCd79L7AfDvu22cleTyru8c4CDwRpK1wL/pe93TwP8DvpBkRTeNcx1/M/8ujZSBrmZV1QzwL4C76U2D7AX+Wdf3V8Avdet/AfwT4L4B23mHXhD/PeBN4K1uPMAd9C6dPDKXf1/f6/4K+BRwNfAa8B+Am6rq+ZHtpNTHD0WlRUiyAfjNqrppqWuR5vIMXRpS923U1+hdFim95xjo0vD+Ob1A/+OFBkpLwSkXSWqEZ+iS1Igl+2LR6tWra3x8fKneXpKWpR07drxWVWPz9S1ZoI+PjzMzM7NUby9Jy1KSQd9odspFklphoEtSIwx0SWqEgS5JjTDQJakRBro0jOlpGB+HM87oPU9PL3VF0rt4gwtpIdPTMDkJhw711vft660DbNmydHVJc3iGLi1k69a/CfMjDh3qtUvvIQa6tJAXX1xcu7REDHRpIRsG3JluULu0RAx0aSHbtsHKlUe3rVzZa5feQwx0aSFbtsDUFGzcCEnveWrKD0T1nuNVLtIwtmwxwPWe5xm6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNGOo69CQvAG8B7wCHq2piTv8VwH8D/nfXdF9V/buRVSlJWtBivlj0C1X12jH6n6yqa0+0IEnS8XHKRZIaMWygF/CtJDuSTA4Yc1mSZ5I8kuSC+QYkmUwyk2Rmdnb2uAqWJM1v2CmXy6vq5SQfBB5N8nxV/Ulf/05gY1UdTHINcD9w/tyNVNUUMAUwMTFRJ1a6JKnfUGfoVfVy9/wq8E1g85z+N6vqYLf8MLAiyeoR1ypJOoYFAz3JB5Kcc2QZ+EXg+3PGnJck3fLmbruvj75cSdIgw0y5/AzwzS6v3wf8QVX9UZKbAapqO3ADcEuSw8DbwI1V5ZSKJJ1CCwZ6Vf0QuGie9u19y3cDd4+2NEnSYnjZoiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDViqEBP8kKS3Ul2JZmZpz9J7kqyN8mzSTaNvlRJ0rEMc0/RI36hql4b0Hc1cH73uAS4p3uWJJ0io5pyuR64t3qeAlYlWTOibUuShjBsoBfwrSQ7kkzO078WeKlvfX/XdpQkk0lmkszMzs4uvlpJ0kDDBvrlVbWJ3tTKrUl+fk5/5nlNvauhaqqqJqpqYmxsbJGlSpKOZahAr6qXu+dXgW8Cm+cM2Q+s71tfB7w8igIlScNZMNCTfCDJOUeWgV8Evj9n2APATd3VLpcCB6rqlZFXK0kaaJirXH4G+GaSI+P/oKr+KMnNAFW1HXgYuAbYCxwCPntyypUkDbJgoFfVD4GL5mnf3rdcwK2jLU2StBh+U1SSGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1IihAz3JmUm+l+ShefquSHIgya7ucftoy5QkLWSYW9AdcRuwB/jpAf1PVtW1J16SJOl4DHWGnmQd8Engd05uOZKk4zXslMudwBeAnxxjzGVJnknySJIL5huQZDLJTJKZ2dnZRZYqSTqWBQM9ybXAq1W14xjDdgIbq+oi4MvA/fMNqqqpqpqoqomxsbHjqVeSNMAwZ+iXA59K8gLwdeDKJF/tH1BVb1bVwW75YWBFktWjLlaSNNiCgV5Vv1pV66pqHLgReKyqPtM/Jsl5SdItb+62+/pJqFeSNMBirnI5SpKbAapqO3ADcEuSw8DbwI1VVaMpUZI0jCxV7k5MTNTMzMySvLckLVdJdlTVxHx9flNUkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktSIoQM9yZlJvpfkoXn6kuSuJHuTPJtk02jLlCQtZDFn6LcBewb0XQ2c3z0mgXtOsC5J0iINFehJ1gGfBH5nwJDrgXur5ylgVZI1I6pRkjSEYc/Q7wS+APxkQP9a4KW+9f1d21GSTCaZSTIzOzu7mDolSQtYMNCTXAu8WlU7jjVsnrZ33X26qqaqaqKqJsbGxhZRpiRpIcOcoV8OfCrJC8DXgSuTfHXOmP3A+r71dcDLI6lQkjSUBQO9qn61qtZV1ThwI/BYVX1mzrAHgJu6q10uBQ5U1SujL1eSNMj7jveFSW4GqKrtwMPANcBe4BDw2ZFUJ0ka2qICvaqeAJ7olrf3tRdw6ygLkyQtjt8UlaRGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYsGOhJzkry3STPJHkuyR3zjLkiyYEku7rH7SenXEnSIMPcgu4vgSur6mCSFcB3kjxSVU/NGfdkVV07+hIlScNYMNC7+4Ue7FZXdI86mUVJkhZvqDn0JGcm2QW8CjxaVU/PM+yyblrmkSQXDNjOZJKZJDOzs7PHX7Uk6V2GCvSqeqeqPgqsAzYn+fCcITuBjVV1EfBl4P4B25mqqomqmhgbGzv+qiVJ77Koq1yq6g3gCeATc9rfrKqD3fLDwIokq0dUoyRpCMNc5TKWZFW3/H7g48Dzc8aclyTd8uZuu6+PvFpJ0kDDXOWyBvj9JGfSC+o/rKqHktwMUFXbgRuAW5IcBt4Gbuw+TJUknSLDXOXyLHDxPO3b+5bvBu4ebWmSpMXwm6KS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYMcwu6s5J8N8kzSZ5Lcsc8Y5LkriR7kzybZNPJKVeSNMgwt6D7S+DKqjqYZAXwnSSPVNVTfWOuBs7vHpcA93TPkqRTZMEz9Oo52K2u6B5z7xd6PXBvN/YpYFWSNaMtVZJ0LEPNoSc5M8ku4FXg0ap6es6QtcBLfev7u7a525lMMpNkZnZ29jhLliTNZ6hAr6p3quqjwDpgc5IPzxmS+V42z3amqmqiqibGxsYWXawkabBFXeVSVW8ATwCfmNO1H1jft74OePlECpOk1kzvnmb8znHOuOMMxu8cZ3r39Ei3P8xVLmNJVnXL7wc+Djw/Z9gDwE3d1S6XAgeq6pWRVipJy9j07mkmH5xk34F9FMW+A/uYfHBypKE+zBn6GuDxJM8C/5PeHPpDSW5OcnM35mHgh8Be4LeBfzWyCiWpAVu/vZVDPz50VNuhHx9i67e3juw9FrxssaqeBS6ep31733IBt46sKklqzIsHXlxU+/Hwm6KSdApsOHfDotqPh4EuSafAtqu2sXLFyqPaVq5Yybarto3sPQx0SToFtly4hanrpth47kZC2HjuRqaum2LLhVtG9h7pTX+fehMTEzUzM7Mk7y1Jy1WSHVU1MV+fZ+iS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJasQwt6Bbn+TxJHuSPJfktnnGXJHkQJJd3eP2k1OuJGmQBe9YBBwGPl9VO5OcA+xI8mhV/dmccU9W1bWjL1GSNIwFz9Cr6pWq2tktvwXsAdae7MIkSYuzqDn0JOP07i/69DzdlyV5JskjSS4YRXGSpOENM+UCQJKzgW8An6uqN+d07wQ2VtXBJNcA9wPnz7ONSWASYMOG0d1HT5I05Bl6khX0wny6qu6b219Vb1bVwW75YWBFktXzjJuqqomqmhgbGzvB0iVJ/Ya5yiXAV4A9VfWlAWPO68aRZHO33ddHWagk6diGmXK5HPhlYHeSXV3bF4ENAFW1HbgBuCXJYeBt4MZaqpuVStJpasFAr6rvAFlgzN3A3aMqSpK0eH5TVJIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhoxzD1F1yd5PMmeJM8luW2eMUlyV5K9SZ5NsunklCuNxvTuacbvHOeMO85g/M5xpndPL3VJ0gkb5p6ih4HPV9XOJOcAO5I8WlV/1jfmauD87nEJcE/3LL3nTO+eZvLBSQ79+BAA+w7sY/LBSQC2XLhlKUuTTsiCZ+hV9UpV7eyW3wL2AGvnDLseuLd6ngJWJVkz8mqlEdj67a1/HeZHHPrxIbZ+e+sSVSSNxqLm0JOMAxcDT8/pWgu81Le+n3eHPkkmk8wkmZmdnV1kqdJovHjgxUW1S8vF0IGe5GzgG8DnqurNud3zvKTe1VA1VVUTVTUxNja2uEqlEdlw7oZFtUvLxVCBnmQFvTCfrqr75hmyH1jft74OePnEy5tjehrGx+GMM3rP036QpcXbdtU2Vq5YeVTbyhUr2XbVtiWqSBqNYa5yCfAVYE9VfWnAsAeAm7qrXS4FDlTVKyOssxfek5Owbx9U9Z4nJw11LdqWC7cwdd0UG8/dSAgbz93I1HVTfiCqZS9V75oZOXpA8g+BJ4HdwE+65i8CGwCqansX+ncDnwAOAZ+tqpljbXdiYqJmZo455Gjj470Qn2vjRnjhheG3I0nLWJIdVTUxX9+Cly1W1XeYf468f0wBtx5feUN6ccAHVoPaJek0s3y+KbphwAdWg9ol6TSzfAJ92zZYefQHWaxc2WuXJC2jQN+yBaamenPmSe95aqrXLkka6qv/7x1bthjgkjTA8jlDlyQdk4EuSY0w0CWpEQa6JDXCQJekRiz41f+T9sbJLDDPd/mXrdXAa0tdxCnmPp8eTrd9fq/v78aqmvfP1S5ZoLcmycygv6/QKvf59HC67fNy3l+nXCSpEQa6JDXCQB+dqaUuYAm4z6eH022fl+3+OocuSY3wDF2SGmGgS1IjDPRFSPK7SV5N8v0B/VckOZBkV/e4/VTXOGpJ1id5PMmeJM8luW2eMUlyV5K9SZ5Nsmkpah2FIfe3qeOc5Kwk303yTLfPd8wzppljDEPv8/I7zlXlY8gH8PPAJuD7A/qvAB5a6jpHvM9rgE3d8jnA/wJ+bs6Ya4BH6N2q8FLg6aWu+yTvb1PHuTtuZ3fLK4CngUtbPcaL2Odld5w9Q1+EqvoT4P8udR2nUlW9UlU7u+W3gD3A2jnDrgfurZ6ngFVJ1pziUkdiyP1tSnfcDnarK7rH3KslmjnGMPQ+LzsG+uhd1v0a90iSC5a6mFFKMg5cTO9spt9a4KW+9f00EILH2F9o7DgnOTPJLuBV4NGqav4YD7HPsMyOs4E+Wjvp/Z2Fi4AvA/cvbTmjk+Rs4BvA56rqzbnd87xkWZ/tLLC/zR3nqnqnqj4KrAM2J/nwnCHNHeMh9nnZHWcDfYSq6s0jv8ZV1cPAiiSrl7isE5ZkBb1wm66q++YZsh9Y37e+Dnj5VNR2Miy0v60eZ4CqegN4AvjEnK6mjnG/Qfu8HI+zgT5CSc5Lkm55M71/39eXtqoT0+3PV4A9VfWlAcMeAG7qroS4FDhQVa+csiJHaJj9be04JxlLsqpbfj/wceD5OcOaOcYw3D4vx+O8vG4SvcSSfI3eJ9+rk+wHfp3ehylU1XbgBuCWJIeBt4Ebq/u4fBm7HPhlYHc33wjwRWAD/PV+P0zvKoi9wCHgs6e+zJEZZn9bO85rgN9Pcia90PrDqnooyc3Q5DGG4fZ52R1nv/ovSY1wykWSGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEb8f+5sFgQD1s6WAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.array([[1,   2], \n",
    "              [2,   4], \n",
    "              [3, 1.5], \n",
    "              [4,   2]])\n",
    "\n",
    "x2 = np.array([[1.2, 1.9], \n",
    "               [2.5, 5.8], \n",
    "               [2.4, 2.0], \n",
    "               [3.7, 2.9]])\n",
    "\n",
    "y = np.array([0, 0, 1, 1]).reshape(-1,1)\n",
    "\n",
    "y2 = np.array([0, 0, 1, 1]).reshape(-1,1)\n",
    "\n",
    "regression = BinaryLogisticalRegression(x, y)\n",
    "regression.trainGD(alpha=0.01, max_iterations = 400)\n",
    "\n",
    "y_predict = regression.predict(x2)\n",
    "\n",
    "print(y_predict)\n",
    "\n",
    "plt.title(\"Dados de treino\")\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    if y[i] == 0:\n",
    "        plt.plot(x[i,0], x[i,1], 'ro')\n",
    "    else:\n",
    "        plt.plot(x[i,0], x[i,1], 'go')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Dados de teste\")\n",
    "\n",
    "for i in range(x2.shape[0]):\n",
    "    if y2[i] == 0:\n",
    "        plt.plot(x2[i,0], x2[i,1], 'ro')\n",
    "    else:\n",
    "        plt.plot(x2[i,0], x2[i,1], 'go')\n",
    "        \n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Predição\")\n",
    "\n",
    "for i in range(x2.shape[0]):\n",
    "    if y_predict[i] == 0:\n",
    "        plt.plot(x2[i,0], x2[i,1], 'ro')\n",
    "    else:\n",
    "        plt.plot(x2[i,0], x2[i,1], 'go')\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76007308",
   "metadata": {},
   "source": [
    "### Regressão softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "8ab266e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassLogisticalRegression:\n",
    "    \n",
    "    # Construtor\n",
    "    def __init__(self, x, y, addOnes = True):\n",
    "        if addOnes:\n",
    "            self.x = np.c_[np.ones(x.shape[0]), x]\n",
    "        else:\n",
    "            self.x = x\n",
    "        self.y = y\n",
    "        self.w = np.zeros((self.x.shape[1], y.shape[1]))\n",
    "        self.MSE = 0.0\n",
    "    \n",
    "    # Getters\n",
    "    def getX(self):\n",
    "        return self.x\n",
    "    \n",
    "    def getY(self):\n",
    "        return self.y\n",
    "    \n",
    "    def getW(self):\n",
    "        return self.w\n",
    "    \n",
    "    def getMSE(self):\n",
    "        return self.MSE\n",
    "    \n",
    "    # Setters\n",
    "    def setXY(self, x, y, addOnes = True):\n",
    "        if addOnes:\n",
    "            x = np.c_[np.ones(x.shape[0]), x]\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.w = np.zeros((self.x.shape[1], y.shape[1]))\n",
    "    \n",
    "    # Métodos\n",
    "    def softmax(self, w, x):\n",
    "        numerator = np.exp(x @ w)\n",
    "        denominator = np.sum(np.exp(x @ w), axis=1).reshape(-1,1)\n",
    "        return numerator/denominator\n",
    "    \n",
    "    def trainGD(self, alpha = 0.1, max_iterations = 100):\n",
    "    \n",
    "        n = self.y.shape[0]\n",
    "        yhat = np.zeros(self.y.shape)\n",
    "        e = np.zeros(self.y.shape)\n",
    "                 \n",
    "        for t in range(max_iterations):\n",
    "            yhat = self.softmax(self.w, self.x)\n",
    "            e = self.y - yhat\n",
    "            self.w = self.w + (alpha/n * (self.x.T @ e))\n",
    "                            \n",
    "        self.MSE = ((e ** 2).sum())/(2*n)\n",
    "    \n",
    "    def test(self):\n",
    "        return self.y - (self.x @ self.w)\n",
    "    \n",
    "    def predict(self, x, addOnes = True):\n",
    "        if addOnes:\n",
    "            x = np.c_[np.ones(x.shape[0]), x]\n",
    "        probabilities = self.softmax(self.w, x)\n",
    "        max_indexes = np.argmax(probabilities, axis=1)\n",
    "        prediction = np.zeros(probabilities.shape)\n",
    "        prediction[np.arange(len(max_indexes)), max_indexes] = 1\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d797af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxIndex(y):\n",
    "    max_index = 0\n",
    "    max_value = y[0]\n",
    "    for clss in range(1, len(y)):\n",
    "        if y[clss] > max_value:\n",
    "            max_value = y[clss]\n",
    "            max_index = clss\n",
    "    return max_index\n",
    "\n",
    "x = np.array([[1.0, 0.25], \n",
    "              [2.0,  0.5], \n",
    "              [3.5,  1.0], \n",
    "              [4.0, 1.25],\n",
    "              [6.0,  2.0],\n",
    "              [6.5,  2.5]])\n",
    "\n",
    "y = np.array([[1, 0, 0], \n",
    "              [1, 0, 0], \n",
    "              [0, 1, 0], \n",
    "              [0, 1, 0],\n",
    "              [0, 0, 1], \n",
    "              [0, 0, 1]])\n",
    "\n",
    "x2 = np.array([[1.1, 0.2], \n",
    "              [2.3,  0.6], \n",
    "              [2.2,  0.8], \n",
    "              [3.9, 1.15],\n",
    "              [7.0,  2.5],\n",
    "              [6.8,  2.8]])\n",
    "\n",
    "y2 = np.array([[1, 0, 0], \n",
    "              [1, 0, 0], \n",
    "              [0, 1, 0], \n",
    "              [0, 1, 0],\n",
    "              [0, 0, 1], \n",
    "              [0, 0, 1]])\n",
    "\n",
    "regression = MulticlassLogisticalRegression(x, y)\n",
    "regression.trainGD(max_iterations = 1000)\n",
    "regression.test()\n",
    "ypredict = regression.predict(x2)\n",
    "\n",
    "\n",
    "plt.title(\"Dados de treino\")\n",
    "for i in range(x.shape[0]):\n",
    "    max_index = getMaxIndex(y[i])\n",
    "    if max_index == 0:\n",
    "        plt.plot(x[i,0], x[i,1], 'ro')\n",
    "    elif max_index == 1:\n",
    "        plt.plot(x[i,0], x[i,1], 'go')\n",
    "    else:\n",
    "        plt.plot(x[i,0], x[i,1], 'bo')\n",
    "plt.show()\n",
    "\n",
    "for i in range(x2.shape[0]):\n",
    "    max_index = getMaxIndex(ypredict[i])\n",
    "    if max_index == 0:\n",
    "        plt.plot(x2[i,0], x2[i,1], 'ro')\n",
    "    elif max_index == 1:\n",
    "        plt.plot(x2[i,0], x2[i,1], 'go')\n",
    "    else:\n",
    "        plt.plot(x2[i,0], x2[i,1], 'bo')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
